import requests
import pandas as pd
from bs4 import BeautifulSoup
import time

API_KEY = '086ad3b3-a4f0-4d5d-9492-f4baa4d6a703'
FROM_DATE = '2020-01-01'
TO_DATE = '2024-12-01'
PAGE_SIZE = 50

TAG_IDS = [
    'world/israel',
    'world/palestinian-territories',
    'world/hamas'
]

def html_to_text(html):
    if not isinstance(html, str):
        return ''
    soup = BeautifulSoup(html, 'html.parser')
    return soup.get_text(separator='\n', strip=True)

def fetch_guardian_by_tag(tag_id):
    all_articles = []
    base_url = 'https://content.guardianapis.com/search'
    params = {
        'from-date': FROM_DATE,
        'to-date': TO_DATE,
        'api-key': API_KEY,
        'show-fields': 'headline,body',
        'show-tags': 'keyword',
        'page-size': PAGE_SIZE,
        'order-by': 'newest',
        'tag': tag_id
    }

    resp = requests.get(base_url, params={**params, 'page': 1})
    if resp.status_code != 200:
        print(f"Tag {tag_id} è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status_code}")
        return []

    total_pages = resp.json()['response']['pages']
    print(f"Tag {tag_id} æ€»é¡µæ•°ï¼š{total_pages}")

    for page in range(1, total_pages + 1):
        print(f"ğŸ“„ æ­£åœ¨æŠ“å– {tag_id} - ç¬¬ {page}/{total_pages} é¡µ ...")
        params['page'] = page
        resp = requests.get(base_url, params=params)
        if resp.status_code != 200:
            print(f"Tag {tag_id} ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{resp.status_code}")
            time.sleep(5)
            continue

        results = resp.json()['response']['results']
        for item in results:
            article = {
                'title': item['fields'].get('headline', ''),
                'body_text': html_to_text(item['fields'].get('body', '')),
                'published_at': item.get('webPublicationDate', ''),
                'url': item.get('webUrl', '')
            }
            if article['body_text'].strip():
                all_articles.append(article)

        time.sleep(0.2)  # é˜²æ­¢è§¦å‘é€Ÿç‡é™åˆ¶

    return all_articles

# æŠ“å–æ‰€æœ‰ tag çš„æ•°æ®
all_results = []
for tag_id in TAG_IDS:
    all_results.extend(fetch_guardian_by_tag(tag_id))

# å»é‡
df = pd.DataFrame(all_results).drop_duplicates(subset=['url'])

# ä¿ç•™æŒ‡å®šåˆ—
df = df[['title', 'body_text', 'published_at', 'url']]

# ä¿å­˜ä¸º CSV
df.to_csv('guardian_tag_filtered_3_keywords.csv', index=False, encoding='utf-8-sig')
print(f'âœ… å·²ä¿å­˜ {len(df)} æ¡è®°å½•ä¸º guardian_tag_filtered_3_keywords.csv')
